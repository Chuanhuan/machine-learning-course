{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Useful starting lines\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "## Classification Using SVM\n",
    "Load dataset. We will re-use the CERN dataset from project 1, available from https://inclass.kaggle.com/c/epfml-project-1/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1. -1. -1. ...,  1. -1. -1.]\n"
     ]
    }
   ],
   "source": [
    "from proj1_helpers import load_csv_data\n",
    "DATA_TRAIN_PATH = './data/train.csv'\n",
    "y, x, ids = load_csv_data(DATA_TRAIN_PATH)\n",
    "print(y)\n",
    "## Note: This is the raw dataset, you can also work with your modified features if you prefer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hinge_loss(z):\n",
    "    return max(0,z)\n",
    "\n",
    "def calculate_cost(y, x, w, lambda_):\n",
    "    \"\"\"compute the full cost (the primal objective), that is loss plus regularizer.\"\"\"\n",
    "    # Here x is the full dataset matrix, and y are the corresponding +1 or -1 labels\n",
    "    z = np.sum(hinge_loss(1-y.dot(x).dot(w))+lambda_/2*np.linalg.norm(w)**2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the (stochastic) subgradient for the n-th summand of the SVM optimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_gradient(y, x, w, lambda_, n):\n",
    "    \"\"\"compute the stochastic gradient of loss plus regularizer.\"\"\"\n",
    "    # Here x is one datapoint, and y is the corresponding +1 or -1 label\n",
    "    x_col = x[n,:].reshape(w.shape)\n",
    "    a = 1-y[n]*x_col.T.dot(w)\n",
    "    if hinge_loss(a[0]) <= 0.00001:\n",
    "        return np.zeros(w.shape)\n",
    "    return -y[n]*(x_col)+lambda_*w  #derivative of cost\n",
    "    # Be careful about the constant N(size) term! The complete objective for SVM is a sum, not an average as in earlier SGD examples!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement stochastic gradient descent: Pick a data point uniformly at random and update w based on the gradient for the n-th summand of the objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, the loss=1.0\n",
      "Current iteration=100, the loss=1.9823473647793265e-05\n",
      "Current iteration=200, the loss=5.6979754598695945e-05\n",
      "Current iteration=300, the loss=90254186.42429535\n",
      "Current iteration=400, the loss=4.744136584339667e-05\n",
      "Current iteration=500, the loss=4.058478073656871e-05\n",
      "Current iteration=600, the loss=0.00010469881504377111\n",
      "Current iteration=700, the loss=5.7827199581732184e-05\n",
      "Current iteration=800, the loss=215133325.54571608\n",
      "Current iteration=900, the loss=0.00016296322569204978\n",
      "Current iteration=1000, the loss=0.00013761313410617365\n",
      "Current iteration=1100, the loss=0.0001284879525856357\n",
      "Current iteration=1200, the loss=0.00014298961374285192\n",
      "Current iteration=1300, the loss=0.00012434091851067875\n",
      "Current iteration=1400, the loss=0.00019521438706902205\n",
      "Current iteration=1500, the loss=0.00018147173540699346\n",
      "Current iteration=1600, the loss=0.00019664035271023305\n",
      "Current iteration=1700, the loss=182746672.36601412\n",
      "Current iteration=1800, the loss=0.00021156280470711585\n",
      "Current iteration=1900, the loss=0.00024413230378734718\n",
      "Current iteration=2000, the loss=0.0002483677274660852\n",
      "Current iteration=2100, the loss=102262718.59162796\n",
      "Current iteration=2200, the loss=295006313.2867992\n",
      "Current iteration=2300, the loss=0.00029134804930714916\n",
      "Current iteration=2400, the loss=0.0003353736949066865\n",
      "Current iteration=2500, the loss=0.0003273863779384276\n",
      "Current iteration=2600, the loss=0.00030761874091176484\n",
      "Current iteration=2700, the loss=0.00030472792287039927\n",
      "Current iteration=2800, the loss=0.00027574853545880027\n",
      "Current iteration=2900, the loss=0.0002854637776953317\n",
      "Current iteration=3000, the loss=0.00030104294236953013\n",
      "Current iteration=3100, the loss=0.0003371129498831049\n",
      "Current iteration=3200, the loss=0.00035034659840742937\n",
      "Current iteration=3300, the loss=0.0004058913322206878\n",
      "Current iteration=3400, the loss=104290853.46844725\n",
      "Current iteration=3500, the loss=350457562.3003779\n",
      "Current iteration=3600, the loss=0.00040456875642320983\n",
      "Current iteration=3700, the loss=0.0004063165795998787\n",
      "Current iteration=3800, the loss=0.00040662808360017883\n",
      "Current iteration=3900, the loss=0.00039543955034077006\n",
      "Current iteration=4000, the loss=0.0003993164669956158\n",
      "Current iteration=4100, the loss=0.0004196629478721441\n",
      "Current iteration=4200, the loss=0.0004188734700892921\n",
      "Current iteration=4300, the loss=299331804.22432935\n",
      "Current iteration=4400, the loss=0.0004473092724950598\n",
      "Current iteration=4500, the loss=0.0004178146409301283\n",
      "Current iteration=4600, the loss=0.0003765129408501067\n",
      "Current iteration=4700, the loss=0.00040577171626574797\n",
      "Current iteration=4800, the loss=0.0004596469942752207\n",
      "Current iteration=4900, the loss=0.00048463576577141034\n",
      "Objective = 0.0005197597615836221\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def sgd_for_svm_demo(y, x):\n",
    "    # ***************************************************\n",
    "    # INSERT YOUR CODE HERE\n",
    "    # classify the data by SGD for SVM: TODO\n",
    "    # ***************************************************\n",
    "    max_iter = 5000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 1.0 / y.shape[0]  # or set to a different value, try cross-validation!\n",
    "    N = len(y)\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        #n = sample one data point uniformly at random data from x\n",
    "        n = random.randint(0, N-1)\n",
    "        loss = calculate_cost(y, x, w, lambda_) \n",
    "        grad = calculate_gradient(y, x, w, lambda_, n)\n",
    "        w = w - gamma*grad        \n",
    "        if iter % 100 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    \n",
    "    print(\"Objective = {l}\".format(l=calculate_cost(y, x, w, lambda_)))\n",
    "\n",
    "sgd_for_svm_demo(y, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coordinate Descent (Ascent) for SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the closed-form update for the n-th variable alpha, in the dual optimization problem, given alpha and the current corresponding w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_coordinate_update(y, x, lambda_, alpha, w, n):\n",
    "    # Here x is one datapoint, and y is the corresponding +1 or -1 label\n",
    "    # n is the coordinate\n",
    "    #slide 6 (find alpha that maximizes g)\n",
    "    #Gradient: np.ones(y.shape) - 1/lambda_*x.dot(Y).dot(alpha) #derivative of XAX.T = 2AX (Francesco)\n",
    "    #setting gradient to 0:\n",
    "    return alpha[n]-1/lambda_ * y[n]*(x[n].T).dot(alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def coordinate_descent_for_svm_demo(y, x):\n",
    "    # ***************************************************\n",
    "    # classify the data by SGD for SVM: TODO\n",
    "    # ***************************************************\n",
    "    max_iter = 10000\n",
    "    gamma = 0.001\n",
    "    lambda_ = 1.0 / y.shape[0]\n",
    "    N = len(y)\n",
    "    w = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    for iter in range(max_iter):\n",
    "        # n = uniformly random data point from x\n",
    "        n = random.randint(0, N-1)\n",
    "        loss = calculate_cost(y, x, w, lambda_) \n",
    "        #we're maximizing! gradient ascent,  slide 9\n",
    "        coords = calculate_coordinate_update(y, x, lambda_, alpha, w)\n",
    "        alpha[n] = coords[n]\n",
    "        Y = np.diagflat(y)\n",
    "        w = update1/lambda_ * x.dot(Y).dot(alpha) #slide 6, w(alpha)\n",
    "        \n",
    "        if iter % 1000 == 0:\n",
    "            print(\"Current iteration={i}, the loss={l}\".format(i=iter, l=loss))\n",
    "    \n",
    "    print(\"Primal objective = {l}\".format(l=calculate_cost(y, x, w, lambda_)))\n",
    "\n",
    "coordinate_descent_for_svm_demo(y, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
