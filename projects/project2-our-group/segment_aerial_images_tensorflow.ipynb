{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*NOTES*\n",
    "- Rule of thumb: per layer, halve dimensions X*Y and double depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\"\"\"\n",
    "Baseline for machine learning project on road segmentation.\n",
    "This simple baseline consits of a CNN with two convolutional+pooling layers with a soft-max loss\n",
    "Credits: Aurelien Lucchi, ETH ZÃ¼rich\n",
    "\"\"\"\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import urllib\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import code\n",
    "import tensorflow.python.platform\n",
    "import numpy\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "NUM_CHANNELS = 3 # RGB images\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 2\n",
    "TRAINING_SIZE = 100 # 100 + 48 augmented\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "BATCH_SIZE = 16\n",
    "RESTORE_MODEL = False # If True, restore existing model instead of training a new one\n",
    "RECORDING_STEP = 1000\n",
    "DATA_AUGMENTATION = False\n",
    "NUM_THREADS = 2\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', './tmp/', \"Directory where to write event logs and checkpoint.\")\n",
    "FLAGS = tf.app.flags.FLAGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing: Data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- data augmentation disabled --\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "\n",
    "EXTRA_IMAGE_IDS = [23,26,27,28,30,32,33,38,42,69,72,73,75,83,88,91]\n",
    "\n",
    "if DATA_AUGMENTATION == True:\n",
    "    img_new_id = 100\n",
    "    img_preffix = \"training/images/satImage_\"\n",
    "    gt_preffix =  \"training/groundtruth/satImage_\"\n",
    "    for i in range(0, len(EXTRA_IMAGE_IDS)):\n",
    "        img_id = EXTRA_IMAGE_IDS[i]\n",
    "        img_filename = img_preffix + (\"%.3d\" % img_id) + \".png\"\n",
    "        gt_filename  = gt_preffix  + (\"%.3d\" % img_id) + \".png\"\n",
    "        if os.path.isfile(img_filename):\n",
    "            im = Image.open(img_filename)\n",
    "            gt = Image.open(gt_filename)\n",
    "            for r in range(1,4): #3 rotations\n",
    "                new_id = 100 + i*3 + r\n",
    "                img_new_name = img_preffix + \"%.3d\" % new_id + \".png\"\n",
    "                gt_new_name =  gt_preffix + \"%.3d\" % new_id + \".png\"\n",
    "                im.rotate(90*r).save(img_new_name)\n",
    "                gt.rotate(90*r).save(gt_new_name)\n",
    "        else:\n",
    "            print ('Oops! File ' + image_filename + ' does not exist')\n",
    "    print(\"-- data augmentation done: added\", len(EXTRA_IMAGE_IDS),\"x3 new images --\")\n",
    "else:\n",
    "    print(\"-- data augmentation disabled --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extract patches from a given image\n",
    "def img_crop(im, w, h, add_intercalated_patches, add_neighboorhood):\n",
    "    list_patches = []\n",
    "    imgwidth = im.shape[0]\n",
    "    imgheight = im.shape[1]\n",
    "    is_2d = len(im.shape) < 3 #for ground_truth images test\n",
    "    \n",
    "    #stepping w/2 and h/2 so that we add some more patches from interleaved intervals\n",
    "    step_h = h\n",
    "    step_w = w\n",
    "    if add_intercalated_patches == True:\n",
    "        step_h = int(h/2)\n",
    "        step_w = int(w/2)\n",
    "            \n",
    "    #standard method, non-overlapping patches\n",
    "    if add_neighboorhood == False:\n",
    "        #the -h+1 and -w+1 avoid the h from copying incomplete patches\n",
    "        for i in range(0,imgheight-h+1,step_h):\n",
    "            for j in range(0,imgwidth-w+1,step_w):\n",
    "                if is_2d:\n",
    "                    im_patch = im[j:j+w, i:i+h]\n",
    "                else:\n",
    "                    im_patch = im[j:j+w, i:i+h, :]\n",
    "                list_patches.append(im_patch)        \n",
    "             \n",
    "    #patches overlapping by a margin, for neighbors analysis\n",
    "    else:\n",
    "        assert CONV_FILTER_SIZES[0] % 2 == 1, \"CONV_FILTER_SIZES[0] must be an ODD number\"\n",
    "        margin = int((CONV_FILTER_SIZES[0]-1)/2) #neighbor pixels per side\n",
    "        new_imgwidth  = int(margin*2+imgwidth)\n",
    "        new_imgheight = int(margin*2+imgheight)\n",
    "        \n",
    "        #create new image with zero-margins and copy original image\n",
    "        if is_2d == True:\n",
    "            im2 = numpy.zeros((new_imgwidth,new_imgheight), dtype=type(im[0][0]))\n",
    "        else:\n",
    "            im2 = numpy.zeros((new_imgwidth,new_imgheight,im.shape[2]), dtype=type(im[0][0][0]))\n",
    "    \n",
    "        for i in range(0,imgheight):\n",
    "            for j in range(0,imgwidth):\n",
    "                im2[margin+j,margin+i] = im[j,i]\n",
    "            \n",
    "        #copy patches, leave margins on both sides\n",
    "        for i in range(margin,new_imgheight-margin-h+1,step_h):\n",
    "            for j in range(margin,new_imgwidth-margin-w+1,step_w):        \n",
    "                if is_2d:\n",
    "                    im_patch = im2[(j-margin):(j+w+margin), (i-margin):(i+h+margin)]\n",
    "                else:\n",
    "                    im_patch = im2[(j-margin):(j+w+margin), (i-margin):(i+h+margin), :]\n",
    "                list_patches.append(im_patch)\n",
    "    return list_patches\n",
    "\n",
    "#return matrix of image patches\n",
    "def extract_data(filename, num_images, phase):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        if phase==1:\n",
    "            image_filename = filename + \"satImage_%.3d\" % i + \".png\"\n",
    "        if phase==2:\n",
    "            image_filename = filename + \"prediction_raw_\" + str(i) + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "                \n",
    "    num_images = len(imgs)\n",
    "    IMG_WIDTH = imgs[0].shape[0]\n",
    "    IMG_HEIGHT = imgs[0].shape[1]\n",
    "    img_patches = [img_crop(imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE, ADD_INTERCALATED_PATCHES, NEIGHBORHOOD_ANALYSIS) for i in range(num_images)]\n",
    "    N_PATCHES_PER_IMAGE = len(img_patches)\n",
    "    data = [img_patches[i][j] for i in range(len(img_patches)) for j in range(len(img_patches[i]))]\n",
    "    return numpy.asarray(data)\n",
    "        \n",
    "# Assign a label to a patch v\n",
    "def value_to_class(v):\n",
    "    foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "    df = numpy.sum(v)\n",
    "    if df > foreground_threshold:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        return [1, 0]\n",
    "\n",
    "# Extract label images\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a 1-hot matrix [image index, label index].\"\"\"\n",
    "    gt_imgs = []\n",
    "    for i in range(1, num_images+1):\n",
    "        image_filename = filename + \"satImage_%.3d\" % i + \".png\"\n",
    "        if os.path.isfile(image_filename):\n",
    "            #print ('Loading ' + image_filename)\n",
    "            img = mpimg.imread(image_filename)\n",
    "            gt_imgs.append(img)\n",
    "        else:\n",
    "            print ('File ' + image_filename + ' does not exist')\n",
    "\n",
    "    num_images = len(gt_imgs)\n",
    "    gt_patches = [img_crop(gt_imgs[i], IMG_PATCH_SIZE, IMG_PATCH_SIZE, ADD_INTERCALATED_PATCHES, False) for i in range(num_images)]\n",
    "    data = numpy.asarray([gt_patches[i][j] for i in range(len(gt_patches)) for j in range(len(gt_patches[i]))])\n",
    "    labels = numpy.asarray([value_to_class(numpy.mean(data[i])) for i in range(len(data))])\n",
    "\n",
    "    # Convert to dense 1-hot representation.\n",
    "    return labels.astype(numpy.float32)\n",
    "\n",
    "\n",
    "#returns percentage of WRONG labels (right ones stored in predictions)\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and 1-hot labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        numpy.sum(numpy.argmax(predictions, 1) == numpy.argmax(labels, 1)) /\n",
    "        predictions.shape[0])\n",
    "\n",
    "# Write predictions from neural network to a file\n",
    "def write_predictions_to_file(predictions, labels, filename):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    file = open(filename, \"w\")\n",
    "    n = predictions.shape[0]\n",
    "    for i in range(0, n):\n",
    "        file.write(max_labels(i) + ' ' + max_predictions(i))\n",
    "    file.close()\n",
    "\n",
    "# Print predictions from neural network\n",
    "def print_predictions(predictions, labels):\n",
    "    max_labels = numpy.argmax(labels, 1)\n",
    "    max_predictions = numpy.argmax(predictions, 1)\n",
    "    print (str(max_labels) + ' ' + str(max_predictions))\n",
    "\n",
    "# Convert array of labels to an image\n",
    "def label_to_img(imgwidth, imgheight, w, h, labels):\n",
    "    array_labels = numpy.zeros([imgwidth, imgheight])\n",
    "    idx = 0\n",
    "    for i in range(0,imgheight,h):\n",
    "        for j in range(0,imgwidth,w):\n",
    "            if labels[idx][0] > 0.5:\n",
    "                l = 1\n",
    "            else:\n",
    "                l = 0\n",
    "            array_labels[j:j+w, i:i+h] = l\n",
    "            idx = idx + 1\n",
    "    return array_labels\n",
    "\n",
    "def img_float_to_uint8(img):\n",
    "    rimg = img - numpy.min(img)\n",
    "    rimg = (rimg / numpy.max(rimg) * PIXEL_DEPTH).round().astype(numpy.uint8)\n",
    "    return rimg\n",
    "\n",
    "def concatenate_images(img, gt_img):\n",
    "    nChannels = len(gt_img.shape)\n",
    "    w = gt_img.shape[0]\n",
    "    h = gt_img.shape[1]\n",
    "    if nChannels == 3:\n",
    "        cimg = numpy.concatenate((img, gt_img), axis=1)\n",
    "    else:\n",
    "        gt_img_3c = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "        gt_img8 = img_float_to_uint8(gt_img)          \n",
    "        gt_img_3c[:,:,0] = gt_img8\n",
    "        gt_img_3c[:,:,1] = gt_img8\n",
    "        gt_img_3c[:,:,2] = gt_img8\n",
    "        img8 = img_float_to_uint8(img)\n",
    "        cimg = numpy.concatenate((img8, gt_img_3c), axis=1)\n",
    "    return cimg\n",
    "\n",
    "def make_img_overlay(img, predicted_img):\n",
    "    w = img.shape[0]\n",
    "    h = img.shape[1]\n",
    "    color_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "    color_mask[:,:,0] = predicted_img*PIXEL_DEPTH\n",
    "\n",
    "    img8 = img_float_to_uint8(img)\n",
    "    background = Image.fromarray(img8, 'RGB').convert(\"RGBA\")\n",
    "    overlay = Image.fromarray(color_mask, 'RGB').convert(\"RGBA\")\n",
    "    new_img = Image.blend(background, overlay, 0.2)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Convolutional Neural Network class definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class CNN:\n",
    "\n",
    "        def __init__(self):\n",
    "            self.cdata = []\n",
    "\n",
    "        def run(self, phase, conv_layers=2):\n",
    "\n",
    "            # Make an image summary for 4d tensor image with index idx\n",
    "            def get_image_summary(img, idx = 0):\n",
    "                #Take img BATCHx16x16x3 --> slice 1x16x16x1 (-1 means \"to all\")\n",
    "                #ie a single patch, all HxV pixels, single column\n",
    "                V = tf.slice(img, (0, 0, 0, idx), (1, -1, -1, 1))\n",
    "                img_w = img.get_shape().as_list()[1] #16: data was BATCH_SIZEx16x16x3\n",
    "                img_h = img.get_shape().as_list()[2]\n",
    "                min_value = tf.reduce_min(V) #gives min number across all dimensions \n",
    "                V = V - min_value  #TRANSLATION: we translate all data (start from 0)\n",
    "                max_value = tf.reduce_max(V)\n",
    "                V = V / (max_value*PIXEL_DEPTH)  #NORMALIZATION: values in 0 to 1\n",
    "                V = tf.reshape(V, (img_w, img_h, 1))\n",
    "                V = tf.transpose(V, (2, 0, 1))\n",
    "                V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "                return V\n",
    "\n",
    "            # Make an image summary for 3d tensor image with index idx\n",
    "            def get_image_summary_3d(img):\n",
    "                V = tf.slice(img, (0, 0, 0), (1, -1, -1))\n",
    "                img_w = img.get_shape().as_list()[1]\n",
    "                img_h = img.get_shape().as_list()[2]\n",
    "                V = tf.reshape(V, (img_w, img_h, 1))\n",
    "                V = tf.transpose(V, (2, 0, 1))\n",
    "                V = tf.reshape(V, (-1, img_w, img_h, 1))\n",
    "                return V         \n",
    "            \n",
    "            # Get prediction for given input image \n",
    "            def get_prediction(img,phase, conv_layers):\n",
    "                data = numpy.asarray(img_crop(img, IMG_PATCH_SIZE, IMG_PATCH_SIZE, False, NEIGHBORHOOD_ANALYSIS))\n",
    "                data_node = tf.constant(data)\n",
    "                output = tf.nn.softmax(model(data_node,phase,conv_layers))\n",
    "                output_prediction = s.run(output)\n",
    "                img_prediction = label_to_img(img.shape[0], img.shape[1], IMG_PATCH_SIZE, IMG_PATCH_SIZE, output_prediction)\n",
    "                return img_prediction\n",
    "\n",
    "            # Get a concatenation of the prediction and groundtruth for given input file\n",
    "            def get_prediction_with_groundtruth(filename, image_idx,phase,conv_layers):\n",
    "                if phase ==1:\n",
    "                    image_filename = filename + \"satImage_%.3d\" % image_idx + \".png\"\n",
    "                if phase == 2:\n",
    "                    image_filename = filename + \"prediction_raw_\" + str(image_idx) + \".png\" \n",
    "                img = mpimg.imread(image_filename)\n",
    "                img_prediction = get_prediction(img,phase,conv_layers)\n",
    "                return concatenate_images(img, img_prediction)\n",
    "                \n",
    "            # Get prediction overlaid on the original image for given input file\n",
    "            def get_prediction_with_overlay(filename, image_idx,phase,conv_layers):\n",
    "                if phase ==1:\n",
    "                    image_filename = filename + \"satImage_%.3d\" % image_idx + \".png\"\n",
    "                if phase == 2:\n",
    "                    image_filename = filename + \"prediction_raw_\" + str(image_idx) + \".png\"\n",
    "                    #image_filename = filename + \"satImage_%.3d\" % image_idx + \".png\" \n",
    "                img = mpimg.imread(image_filename)\n",
    "\n",
    "                img_prediction = get_prediction(img,phase, conv_layers)\n",
    "                oimg = make_img_overlay(img, img_prediction)\n",
    "\n",
    "                return oimg\n",
    "\n",
    "            # We will replicate the model structure for the training subgraph, as well\n",
    "            # as the evaluation subgraphs, while sharing the trainable parameters.\n",
    "            def model(data, phase, conv_layers, train=False):\n",
    "                \"\"\"The Model definition.\"\"\"\n",
    "                convs = [None] * conv_layers\n",
    "                relus = [None] * conv_layers\n",
    "                pools = [None] * conv_layers\n",
    "            \n",
    "                #define all convolational networks layers\n",
    "                for i in range (0, conv_layers):\n",
    "                    if i==0:\n",
    "                        if NEIGHBORHOOD_ANALYSIS == True:\n",
    "                            padding_string = 'VALID'\n",
    "                        else:\n",
    "                            padding_string = 'SAME'\n",
    "                        convs[i] = tf.nn.conv2d(data,    ###input is data : BATCH_SIZEx16x16x3\n",
    "                                        conv_weights[i], #### 5x5x3x32\n",
    "                                        strides=[1, 1, 1, 1],\n",
    "                                        padding=padding_string)\n",
    "                    else:\n",
    "                        convs[i] = tf.nn.conv2d(pools[i-1], #input is previous layers output\n",
    "                                    conv_weights[i],\n",
    "                                    strides=[1, 1, 1, 1],\n",
    "                                    padding='SAME')\n",
    "                \n",
    "                    # activity funtion: bias and rectified linear non-linearity. relu(w.T*x+b)\n",
    "                    relus[i] = tf.nn.relu(tf.nn.bias_add(convs[i], conv_biases[i]))\n",
    "\n",
    "                    #pooling: best of CONV_POOLING_STRIDE results from every X and Y output\n",
    "                    pools[i] = tf.nn.max_pool(relus[i],\n",
    "                                      ksize  =[1, POOL_FILTER_STRIDES[i], POOL_FILTER_STRIDES[i], 1],\n",
    "                                      strides=[1, POOL_FILTER_STRIDES[i], POOL_FILTER_STRIDES[i], 1],\n",
    "                                      padding='SAME') \n",
    "                    \n",
    "                # Reshape the feature map cuboid into a 2D matrix to feed it to the fully connected layers.\n",
    "                last_pool = pools[conv_layers-1];\n",
    "                pool_shape = last_pool.get_shape().as_list()\n",
    "                reshape = tf.reshape(\n",
    "                    last_pool, #16x4x4x64\n",
    "                    [pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]]) #[16, 16*16*64]\n",
    "\n",
    "                # During training, output data types and sizes\n",
    "                if train==True :#or train==False:\n",
    "                    print (\"== INFORMATION ON DIMENSIONALITY (train =\", str(train),\"):\")\n",
    "                    print (\"-- data: \", str(data.get_shape()))\n",
    "                    for i in range(0,conv_layers):\n",
    "                        print (\"-- convs[\"+str(i)+\"]:\", str(convs[i].get_shape()))\n",
    "                        print (\"-- conv_biases[\"+str(i)+\"]:\", str(conv_biases[i].get_shape()))\n",
    "                        print (\"-- conv_weights[\"+str(i)+\"]:\", str(conv_weights[i].get_shape()))\n",
    "                        print (\"-- relus[\"+str(i)+\"]:\", str(relus[i].get_shape()))\n",
    "                        print (\"-- relus[\"+str(i)+\"]:\", str(relus[i].get_shape()))\n",
    "                        print (\"-- pools[\"+str(i)+\"]:\", str(pools[i].get_shape()))\n",
    "                    print (\"-- reshape:\", str(reshape.get_shape()))\n",
    "                    print (\"-- fc1_weights:\", str(fc1_weights.get_shape()))\n",
    "                    print (\"-- fc2_weights:\", str(fc2_weights.get_shape()))\n",
    "                \n",
    "                # Fully connected layer. Note that the '+' operation automatically broadcasts the biases.\n",
    "                hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "\n",
    "                # Add a 50% dropout during training only. Dropout also scales\n",
    "                # activations such that no rescaling is needed at evaluation time.\n",
    "                if train and DROPOUT_RATE>0:\n",
    "                    print(\"Performing\", DROPOUT_RATE, \"dropout during training.\")\n",
    "                    hidden = tf.nn.dropout(hidden, DROPOUT_RATE, seed=SEED)\n",
    "                out = tf.matmul(hidden, fc2_weights) + fc2_biases\n",
    "\n",
    "                if train==True:\n",
    "                    print (\"-- hidden:\", str(hidden.get_shape()))\n",
    "                    print (\"-- out:\", str(out.get_shape()))\n",
    " \n",
    "                if train == True:\n",
    "                    summary_id = '_0'\n",
    "                    s_data = get_image_summary(data) #from docs: 3 channels so it's interpreted as RGB\n",
    "                    filter_summary0 = tf.image_summary('summary_data' + summary_id, s_data)\n",
    "                    s_convs = [None] * conv_layers\n",
    "                    filter_summaries = [None] * conv_layers *2\n",
    "                    s_pools = [None] * conv_layers\n",
    "                    for i in range (0, conv_layers):\n",
    "                        s_convs[i] = get_image_summary(convs[i])\n",
    "                        filter_summaries[i]   = tf.image_summary('summary_conv' + str(i) + summary_id, s_convs[i])\n",
    "                        s_pools[i] = get_image_summary(pools[i])\n",
    "                        filter_summaries[i+1] = tf.image_summary('summary_pool' + str(i) + summary_id, s_pools[i])\n",
    "                return out\n",
    "\n",
    "            #reset the graph from previous executions\n",
    "            tf.reset_default_graph()\n",
    "            \n",
    "            #create all convolutional network layers\n",
    "            conv_weights = [None] * conv_layers\n",
    "            conv_biases  = [None] * conv_layers\n",
    "            for i in range (0, conv_layers):\n",
    "                assert CONV_FILTER_SIZES[i] % 2 == 1, \"CONV_FILTER_SIZES must only contain ODD sizes numbers\"\n",
    "                if i == 0 :\n",
    "                    conv_weights[i] = tf.Variable(\n",
    "                        tf.truncated_normal([CONV_FILTER_SIZES[i], CONV_FILTER_SIZES[i], NUM_CHANNELS, CONV_FILTER_DEPTHS[i]],\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED)) #NOTE: this randomness allows the weights not to be started as zero (so that we can start training.. otherwise derivative is 0)\n",
    "                    conv_biases[i] = tf.Variable(tf.zeros([CONV_FILTER_DEPTHS[i]]))  #the +b in the equation above\n",
    "\n",
    "                else:\n",
    "                    conv_weights[i] = tf.Variable(\n",
    "                        tf.truncated_normal([CONV_FILTER_SIZES[i], CONV_FILTER_SIZES[i], CONV_FILTER_DEPTHS[i-1], CONV_FILTER_DEPTHS[i]],\n",
    "                                stddev=0.1,\n",
    "                                seed=SEED))  #each of 64 outputs of conv2 will be connected to 64 nodes in upper layer\n",
    "                    conv_biases[i] = tf.Variable(tf.constant(0.1, shape=[CONV_FILTER_DEPTHS[i]]))  #TODO why is it a constant?\n",
    "            \n",
    "            #create the two fully connected layers\n",
    "\n",
    "            #calculate the total pixels, taking into account all pixels discarded by strides in all layers\n",
    "            fc1_pixel_size = IMG_PATCH_SIZE\n",
    "            for i in range(0, conv_layers):\n",
    "                #make sure strides and patches size are divisible\n",
    "                assert IMG_PATCH_SIZE / POOL_FILTER_STRIDES[i] % 1 == 0, \"IMG_PATCH_SIZE / POOL_FILTER_STRIDES[%r] is not an integer!\" % i\n",
    "                fc1_pixel_size /= POOL_FILTER_STRIDES[i]\n",
    "                \n",
    "            fc1_weights = tf.Variable( \n",
    "                tf.truncated_normal([int(fc1_pixel_size*fc1_pixel_size*CONV_FILTER_DEPTHS[conv_layers-1]), FC1_WEIGHTS_DEPTH],\n",
    "                                    stddev=0.1,\n",
    "                                    seed=SEED))\n",
    "            fc1_biases = tf.Variable(tf.constant(0.1, shape=[FC1_WEIGHTS_DEPTH]))\n",
    "            fc2_weights = tf.Variable(\n",
    "                tf.truncated_normal([FC1_WEIGHTS_DEPTH, NUM_LABELS],\n",
    "                                    stddev=0.1,\n",
    "                                    seed=SEED))\n",
    "            fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS]))\n",
    "\n",
    "            print(phase, \": extract_data...\")\n",
    "            if phase == 1:\n",
    "                train_data_filename = 'training/images/'\n",
    "                train_data = extract_data(train_data_filename, TRAINING_SIZE,phase)\n",
    "\n",
    "            if phase == 2:\n",
    "                train_data_filename = \"predictions_training/\"\n",
    "                train_data = extract_data(train_data_filename, TRAINING_SIZE,phase)\n",
    "\n",
    "            # Extract labels into numpy arrays.\n",
    "            print(phase, \": extract_labels...\")\n",
    "            train_labels_filename = 'training/groundtruth/' \n",
    "            train_labels = extract_labels(train_labels_filename, TRAINING_SIZE)\n",
    "\n",
    "            num_epochs = NUM_EPOCHS #iterations count\n",
    "\n",
    "            c0 = 0 #count of tiles labelled as 0\n",
    "            c1 = 0 #... as 1\n",
    "            for i in range(len(train_labels)):\n",
    "                if train_labels[i][0] == 1:\n",
    "                    c0 = c0 + 1\n",
    "                else:\n",
    "                    c1 = c1 + 1\n",
    "\n",
    "            #We are training on the same number of 1s and 0s, to avoid training data being biased!\n",
    "            print (phase,': before balancing: number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1) + ', random==' + str(RANDOMIZE_INPUT_PATCHES))\n",
    "            print(\"-- train_data   (before): \", numpy.shape(train_data))\n",
    "            print(\"-- train_labels (before): \", numpy.shape(train_labels.shape))\n",
    "            min_c = min(c0, c1)\n",
    "            idx0 = [i for i, j in enumerate(train_labels) if j[0] == 1]\n",
    "            idx1 = [i for i, j in enumerate(train_labels) if j[1] == 1]\n",
    "            \n",
    "            #We try to randomize the picking of patches (its discarding \"non-road\" of last pics only...)\n",
    "            if RANDOMIZE_INPUT_PATCHES == True:\n",
    "                if (SEED != None):\n",
    "                    numpy.random.seed(SEED)\n",
    "                numpy.random.shuffle(idx0)\n",
    "                numpy.random.shuffle(idx1)\n",
    "            new_indices = idx0[0:min_c] + idx1[0:min_c]\n",
    "            train_data = train_data[new_indices,:,:,:]\n",
    "            print(\"-- train_data    (after): \", numpy.shape(train_data))\n",
    "            train_labels = train_labels[new_indices]\n",
    "            print(\"-- train_labels  (after): \", numpy.shape(train_labels.shape))\n",
    "            train_size = train_labels.shape[0]\n",
    "\n",
    "            #counts number of c0 and c1\n",
    "            c0 = 0\n",
    "            c1 = 0\n",
    "            for i in range(len(train_labels)):\n",
    "                if train_labels[i][0] == 1:\n",
    "                    c0 = c0 + 1\n",
    "                else:\n",
    "                    c1 = c1 + 1\n",
    "            print (phase, ': after balancing: Number of data points per class: c0 = ' + str(c0) + ' c1 = ' + str(c1) + ', random==' + str(RANDOMIZE_INPUT_PATCHES))\n",
    "\n",
    "            # This is where training samples and labels are fed to the graph.\n",
    "            # These placeholder nodes will be fed a batch of training data at each\n",
    "            # training step using the {feed_dict} argument to the Run() call below.\n",
    "\n",
    "            patch_size = IMG_PATCH_SIZE\n",
    "            if NEIGHBORHOOD_ANALYSIS==True:\n",
    "                patch_size += CONV_FILTER_SIZES[0]-1 #add the margin pixels for both sides\n",
    "    \n",
    "            train_data_node = tf.placeholder(\n",
    "                tf.float32,\n",
    "                shape=(BATCH_SIZE, patch_size, patch_size, NUM_CHANNELS))\n",
    "            train_labels_node = tf.placeholder(tf.float32, shape=(BATCH_SIZE, NUM_LABELS))\n",
    "            train_all_data_node = tf.constant(train_data) #converting train_data to tensorflow variable\n",
    "            print(\"-- train_all_data_node:\", str(train_all_data_node.get_shape()))\n",
    "\n",
    "            # Training computation: logits + cross-entropy loss.\n",
    "            print(\"-- train_data_node:\", train_data_node.get_shape())\n",
    "            logits = model(train_data_node, phase, conv_layers, True) # BATCH_SIZE*16x16x3\n",
    "            print(\"-- logits =\", str(logits.get_shape()))\n",
    "            print(\"-- train_labels_node = \", str(train_labels_node.get_shape()))\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(   #softmax cross entropy is the loss function\n",
    "                logits, train_labels_node))\n",
    "            tf.scalar_summary('loss', loss)\n",
    "\n",
    "            #set up parameters for nodes\n",
    "            all_params_node  = []\n",
    "            for i in range (0, conv_layers):\n",
    "                all_params_node.append(conv_weights[i])\n",
    "                all_params_node.append(conv_biases[i])\n",
    "            all_params_node.append(fc1_weights)\n",
    "            all_params_node.append(fc1_biases)\n",
    "            all_params_node.append(fc2_weights)\n",
    "            all_params_node.append(fc2_biases)\n",
    "\n",
    "            all_params_names = []\n",
    "            for i in range (0, conv_layers):\n",
    "                all_params_names.append('conv_weights['+str(i)+']')\n",
    "                all_params_names.append('conv_biases['+str(i)+']')\n",
    "            all_params_names.append('fc1_weights')\n",
    "            all_params_names.append('fc1_biases')\n",
    "            all_params_names.append('fc2_weights')\n",
    "            all_params_names.append('fc2_biases')\n",
    "            all_grads_node = tf.gradients(loss, all_params_node)\n",
    "            all_grad_norms_node = [None] * conv_layers\n",
    "            for i in range(0, len(all_grads_node)):\n",
    "                norm_grad_i = tf.global_norm([all_grads_node[i]])\n",
    "                all_grad_norms_node.append(norm_grad_i)\n",
    "                tf.scalar_summary(all_params_names[i], norm_grad_i)\n",
    "\n",
    "            # L2 regularization for the fully connected parameters.\n",
    "            #### avoid extrploding weights (\"it only makes changes to the weights if they will really make a difference\")\n",
    "            regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "                            tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "\n",
    "            # Add the regularization term to the loss.\n",
    "            loss += 5e-4 * regularizers\n",
    "\n",
    "            # Optimizer: set up a variable that's incremented once per batch and controls the learning rate decay.\n",
    "            batch = tf.Variable(0)\n",
    "            # Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "            learning_rate = tf.train.exponential_decay(\n",
    "                LEARNING_RATE,       # Base learning rate.\n",
    "                batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                train_size,          # Decay step.\n",
    "                DECAY_RATE,          # Decay of the step size\n",
    "                staircase=True)\n",
    "            tf.scalar_summary('learning_rate', learning_rate)\n",
    "\n",
    "            # Use simple momentum for the optimization.\n",
    "            optimizer = tf.train.MomentumOptimizer(learning_rate,0.0).minimize(loss, global_step=batch)\n",
    "\n",
    "            # Predictions for the minibatch, validation set and test set.\n",
    "            train_prediction = tf.nn.softmax(logits)\n",
    "            \n",
    "            # We'll compute them only once in a while by calling their {eval()} method.\n",
    "            train_all_prediction = tf.nn.softmax(model(train_all_data_node,phase, conv_layers))\n",
    "\n",
    "            # Add ops to save and restore all the variables.\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            # Create a local session to run this computation.\n",
    "            with tf.Session(config=tf.ConfigProto(intra_op_parallelism_threads=NUM_THREADS)) as s:\n",
    "\n",
    "                if RESTORE_MODEL:\n",
    "                    # Restore variables from disk.\n",
    "                    saver.restore(s, FLAGS.train_dir + \"/model_phase_\"+str(phase)+\".ckpt\")\n",
    "\n",
    "                else:\n",
    "                    # Run all the initializers to prepare the trainable parameters.\n",
    "                    #tf.initialize_all_variables().run()\n",
    "                    tf.global_variables_initializer().run()\n",
    "\n",
    "                    # Build the summary operation based on the TF collection of Summaries.\n",
    "                    summary_op = tf.summary.merge_all()\n",
    "                    summary_writer = tf.train.SummaryWriter(FLAGS.train_dir,\n",
    "                                                            graph=s.graph)\n",
    "                                                            #graph_def=s.graph_def)\n",
    "                    # Loop through training steps.\n",
    "                    print ('Initialized: total number of iterations = ' + str(int(num_epochs * train_size / BATCH_SIZE)))\n",
    "\n",
    "                    training_indices = range(train_size)\n",
    "\n",
    "                    for iepoch in range(num_epochs):\n",
    "\n",
    "                        # Permute training indices\n",
    "                        perm_indices = numpy.random.permutation(training_indices)\n",
    "\n",
    "                        for step in range (int(train_size / BATCH_SIZE)):\n",
    "\n",
    "                            offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "                            batch_indices = perm_indices[offset:(offset + BATCH_SIZE)]\n",
    "\n",
    "                            # Compute the offset of the current minibatch in the data.\n",
    "                            # Note that we could use better randomization across epochs.\n",
    "                            batch_data = train_data[batch_indices, :, :, :]\n",
    "                            batch_labels = train_labels[batch_indices]\n",
    "                            # This dictionary maps the batch data (as a numpy array) to the\n",
    "                            # node in the graph is should be fed to.\n",
    "                            feed_dict = {train_data_node: batch_data,\n",
    "                                         train_labels_node: batch_labels}\n",
    "\n",
    "                            if step % RECORDING_STEP == 0:\n",
    "                                summary_str, _, l, lr, predictions = s.run(\n",
    "                                    [summary_op, optimizer, loss, learning_rate, train_prediction],\n",
    "                                    feed_dict=feed_dict)\n",
    "                                #summary_str = s.run(summary_op, feed_dict=feed_dict) #TODO uncomment this? what does it do?\n",
    "                                summary_writer.add_summary(summary_str, step)\n",
    "                                summary_writer.flush()\n",
    "\n",
    "                                # print_predictions(predictions, batch_labels)\n",
    "                                print (datetime.datetime.now().strftime(\"%H:%M:%S\"), 'Epoch: ', iepoch,'.',step,', minibatch loss: %.3f' % (l), ', Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "                                sys.stdout.flush()\n",
    "                            else:\n",
    "                                # Run the graph and fetch some of the nodes.\n",
    "                                _, l, lr, predictions = s.run(\n",
    "                                    [optimizer, loss, learning_rate, train_prediction],\n",
    "                                    feed_dict=feed_dict)\n",
    "\n",
    "                        # Save the variables to disk.\n",
    "                        save_path = saver.save(s, FLAGS.train_dir + \"/model_phase_\"+str(phase)+\".ckpt\")\n",
    "                        print(\"Model saved in file: %s\" % save_path)\n",
    "\n",
    "\n",
    "                prediction_training_dir = \"./predictions/\"\n",
    "                print (\"Running prediction on training set, outputing\", TRAINING_SIZE,\"files\")\n",
    "                if not os.path.isdir(prediction_training_dir):\n",
    "                    os.mkdir(prediction_training_dir)\n",
    "                    \n",
    "                for i in range(1, TRAINING_SIZE+1):\n",
    "                    if phase ==1:\n",
    "                        image_filename = train_data_filename + \"satImage_%.3d\" % i + \".png\"\n",
    "                    if phase == 2:\n",
    "                        image_filename = train_data_filename + \"prediction_raw_\" + str(i) + \".png\" \n",
    "\n",
    "                    pimg = get_prediction_with_groundtruth(train_data_filename,i,phase,conv_layers)\n",
    "\n",
    "                    rimg = mpimg.imread(image_filename)\n",
    "                    rimg_prediction = get_prediction(rimg,phase, conv_layers)\n",
    "                    #convert from 2D array 1/0 to RGB\n",
    "                    w = rimg_prediction.shape[0]\n",
    "                    h = rimg_prediction.shape[1]\n",
    "                    rimg_mask = numpy.zeros((w, h, 3), dtype=numpy.uint8)\n",
    "                    rimg_mask[:,:,0] = rimg_prediction*PIXEL_DEPTH\n",
    "                    rimg_mask[:,:,1] = rimg_prediction*PIXEL_DEPTH\n",
    "                    rimg_mask[:,:,2] = rimg_prediction*PIXEL_DEPTH\n",
    "                    rimg_final = Image.fromarray(rimg_mask, 'RGB')    \n",
    "                    \n",
    "                    oimg = get_prediction_with_overlay(train_data_filename,i,phase,conv_layers)\n",
    "                    \n",
    "                    if phase == 1:\n",
    "                        Image.fromarray(pimg).save(prediction_training_dir + \"prediction_\" + str(i) + \".png\")\n",
    "                        rimg_final.save(prediction_training_dir + \"prediction_raw_\" + str(i) + \".png\")\n",
    "                        oimg.save(prediction_training_dir + \"overlay_\" + str(i) + \".png\")\n",
    "                    if phase == 2:\n",
    "                        Image.fromarray(pimg).save(prediction_training_dir + \"prediction_2_\" + str(i) + \".png\")\n",
    "                        rimg_final.save(prediction_training_dir + \"prediction_raw_2_\" + str(i) + \".png\")\n",
    "                        oimg.save(prediction_training_dir + \"overlay_2_\" + str(i) + \".png\")\n",
    "\n",
    "            print(\"-- job done --\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : extract_data...\n",
      "1 : extract_labels...\n",
      "1 : before balancing: number of data points per class: c0 = 46309 c1 = 16191, random==False\n",
      "-- train_data   (before):  (62500, 16, 16, 3)\n",
      "-- train_labels (before):  (2,)\n",
      "-- train_data    (after):  (32382, 16, 16, 3)\n",
      "-- train_labels  (after):  (2,)\n",
      "1 : after balancing: Number of data points per class: c0 = 16191 c1 = 16191, random==False\n",
      "-- train_all_data_node: (32382, 16, 16, 3)\n",
      "-- train_data_node: (16, 16, 16, 3)\n",
      "== INFORMATION ON DIMENSIONALITY (train = True ):\n",
      "-- data:  (16, 16, 16, 3)\n",
      "-- convs[0]: (16, 16, 16, 32)\n",
      "-- conv_biases[0]: (32,)\n",
      "-- conv_weights[0]: (5, 5, 3, 32)\n",
      "-- relus[0]: (16, 16, 16, 32)\n",
      "-- relus[0]: (16, 16, 16, 32)\n",
      "-- pools[0]: (16, 8, 8, 32)\n",
      "-- convs[1]: (16, 8, 8, 64)\n",
      "-- conv_biases[1]: (64,)\n",
      "-- conv_weights[1]: (5, 5, 32, 64)\n",
      "-- relus[1]: (16, 8, 8, 64)\n",
      "-- relus[1]: (16, 8, 8, 64)\n",
      "-- pools[1]: (16, 4, 4, 64)\n",
      "-- reshape: (16, 1024)\n",
      "-- fc1_weights: (1024, 512)\n",
      "-- fc2_weights: (512, 2)\n",
      "-- hidden: (16, 512)\n",
      "-- out: (16, 2)\n",
      "-- logits = (16, 2)\n",
      "-- train_labels_node =  (16, 2)\n",
      "Initialized: total number of iterations = 4047\n",
      "01:57:22 Epoch:  0 . 0 , minibatch loss: 1.725 , Minibatch error: 18.8%\n",
      "01:57:50 Epoch:  0 . 1000 , minibatch loss: 1.685 , Minibatch error: 31.2%\n",
      "01:58:25 Epoch:  0 . 2000 , minibatch loss: 1.520 , Minibatch error: 25.0%\n",
      "Model saved in file: ./tmp//model_phase_1.ckpt\n",
      "01:58:27 Epoch:  1 . 0 , minibatch loss: 1.549 , Minibatch error: 37.5%\n",
      "01:58:56 Epoch:  1 . 1000 , minibatch loss: 1.539 , Minibatch error: 31.2%\n",
      "01:59:24 Epoch:  1 . 2000 , minibatch loss: 1.365 , Minibatch error: 31.2%\n",
      "Model saved in file: ./tmp//model_phase_1.ckpt\n",
      "Running prediction on training set, outputing 100 files\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-07388b2983bf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m#execute phase 1 (train inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mcnn1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcnn1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONV_LAYERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-6478babe8e7a>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, phase, conv_layers)\u001b[0m\n\u001b[1;32m    413\u001b[0m                     \u001b[0mrimg_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrimg_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                     \u001b[0moimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction_with_overlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6478babe8e7a>\u001b[0m in \u001b[0;36mget_prediction_with_overlay\u001b[0;34m(filename, image_idx, phase, conv_layers)\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mimg_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_prediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0moimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_img_overlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6478babe8e7a>\u001b[0m in \u001b[0;36mget_prediction\u001b[0;34m(img, phase, conv_layers)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mdata_node\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_node\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m                 \u001b[0moutput_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m                 \u001b[0mimg_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabel_to_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_prediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mimg_prediction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    765\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 766\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    767\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    962\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 964\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    965\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    966\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1014\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1015\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1019\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1020\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1022\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    997\u001b[0m                 run_metadata):\n\u001b[1;32m    998\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m         return tf_session.TF_Run(session, options,\n",
      "\u001b[0;32m/Users/bmagalha/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m           tf_session.TF_ExtendGraph(\n\u001b[0;32m-> 1048\u001b[0;31m               self._session, graph_def.SerializeToString(), status)\n\u001b[0m\u001b[1;32m   1049\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_opened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## CNN settings:\n",
    "\n",
    "ADD_INTERCALATED_PATCHES = False #creates approx 4x more data by adding intercalated patches during traning\n",
    "NEIGHBORHOOD_ANALYSIS = False #if false: patch size of single patch with 'same' passing;\n",
    "                              #if true: patch size addes extra pixels and 'valid' padding\n",
    "                              #and adds (CONV_FILTER_SIZES[0]-1)/2 pixels on each size\n",
    "\n",
    "RANDOMIZE_INPUT_PATCHES = False\n",
    "\n",
    "CONV_LAYERS=2\n",
    "\n",
    "if NEIGHBORHOOD_ANALYSIS == True:\n",
    "    NEIGHBOR_PIXELS = 4\n",
    "    #analyses area of IMG_PATCH_SIZE and NEIGHBOR_PIXELS pixels on all sides,\n",
    "    #but learns from the classification given by the IMG_PATCH_SIZE area only\n",
    "    #(ie classifies a patch of pixels by also taking into account neighbor pixels)\n",
    "    CONV_FILTER_SIZES = [NEIGHBOR_PIXELS*2+1, 5, 5, 5]\n",
    "else:\n",
    "    CONV_FILTER_SIZES = [5, 5, 5, 5]\n",
    "\n",
    "IMG_PATCH_SIZE = 16  #4,8,12,16\n",
    "CONV_FILTER_DEPTHS = [32, 64, 128, 256] #depth of conv_weights[i]\n",
    "POOL_FILTER_STRIDES = [2, 2, 2, 2] #stride for pooling\n",
    "FC1_WEIGHTS_DEPTH = 512 #depth of weights in fully connected 1 (before out)\n",
    "\n",
    "#Learning settings\n",
    "DROPOUT_RATE = 0 #amount of nodes we drop during training (0 for 'no dropout')\n",
    "LEARNING_RATE = 0.03\n",
    "DECAY_RATE = 0.95 #decay of step size of gradient descent\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "#execute phase 1 (train inputs)\n",
    "cnn1 = CNN()\n",
    "cnn1.run(phase=1, conv_layers=CONV_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PHASE 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : extract_data...\n",
      "File predictions_training/prediction_raw_6.png does not exist\n",
      "File predictions_training/prediction_raw_7.png does not exist\n",
      "File predictions_training/prediction_raw_8.png does not exist\n",
      "File predictions_training/prediction_raw_9.png does not exist\n",
      "File predictions_training/prediction_raw_10.png does not exist\n",
      "File predictions_training/prediction_raw_11.png does not exist\n",
      "File predictions_training/prediction_raw_12.png does not exist\n",
      "File predictions_training/prediction_raw_13.png does not exist\n",
      "File predictions_training/prediction_raw_14.png does not exist\n",
      "File predictions_training/prediction_raw_15.png does not exist\n",
      "File predictions_training/prediction_raw_16.png does not exist\n",
      "File predictions_training/prediction_raw_17.png does not exist\n",
      "File predictions_training/prediction_raw_18.png does not exist\n",
      "File predictions_training/prediction_raw_19.png does not exist\n",
      "File predictions_training/prediction_raw_20.png does not exist\n",
      "File predictions_training/prediction_raw_21.png does not exist\n",
      "File predictions_training/prediction_raw_22.png does not exist\n",
      "File predictions_training/prediction_raw_23.png does not exist\n",
      "File predictions_training/prediction_raw_24.png does not exist\n",
      "File predictions_training/prediction_raw_25.png does not exist\n",
      "File predictions_training/prediction_raw_26.png does not exist\n",
      "File predictions_training/prediction_raw_27.png does not exist\n",
      "File predictions_training/prediction_raw_28.png does not exist\n",
      "File predictions_training/prediction_raw_29.png does not exist\n",
      "File predictions_training/prediction_raw_30.png does not exist\n",
      "File predictions_training/prediction_raw_31.png does not exist\n",
      "File predictions_training/prediction_raw_32.png does not exist\n",
      "File predictions_training/prediction_raw_33.png does not exist\n",
      "File predictions_training/prediction_raw_34.png does not exist\n",
      "File predictions_training/prediction_raw_35.png does not exist\n",
      "File predictions_training/prediction_raw_36.png does not exist\n",
      "File predictions_training/prediction_raw_37.png does not exist\n",
      "File predictions_training/prediction_raw_38.png does not exist\n",
      "File predictions_training/prediction_raw_39.png does not exist\n",
      "File predictions_training/prediction_raw_40.png does not exist\n",
      "File predictions_training/prediction_raw_41.png does not exist\n",
      "File predictions_training/prediction_raw_42.png does not exist\n",
      "File predictions_training/prediction_raw_43.png does not exist\n",
      "File predictions_training/prediction_raw_44.png does not exist\n",
      "File predictions_training/prediction_raw_45.png does not exist\n",
      "File predictions_training/prediction_raw_46.png does not exist\n",
      "File predictions_training/prediction_raw_47.png does not exist\n",
      "File predictions_training/prediction_raw_48.png does not exist\n",
      "File predictions_training/prediction_raw_49.png does not exist\n",
      "File predictions_training/prediction_raw_50.png does not exist\n",
      "File predictions_training/prediction_raw_51.png does not exist\n",
      "File predictions_training/prediction_raw_52.png does not exist\n",
      "File predictions_training/prediction_raw_53.png does not exist\n",
      "File predictions_training/prediction_raw_54.png does not exist\n",
      "File predictions_training/prediction_raw_55.png does not exist\n",
      "File predictions_training/prediction_raw_56.png does not exist\n",
      "File predictions_training/prediction_raw_57.png does not exist\n",
      "File predictions_training/prediction_raw_58.png does not exist\n",
      "File predictions_training/prediction_raw_59.png does not exist\n",
      "File predictions_training/prediction_raw_60.png does not exist\n",
      "File predictions_training/prediction_raw_61.png does not exist\n",
      "File predictions_training/prediction_raw_62.png does not exist\n",
      "File predictions_training/prediction_raw_63.png does not exist\n",
      "File predictions_training/prediction_raw_64.png does not exist\n",
      "File predictions_training/prediction_raw_65.png does not exist\n",
      "File predictions_training/prediction_raw_66.png does not exist\n",
      "File predictions_training/prediction_raw_67.png does not exist\n",
      "File predictions_training/prediction_raw_68.png does not exist\n",
      "File predictions_training/prediction_raw_69.png does not exist\n",
      "File predictions_training/prediction_raw_70.png does not exist\n",
      "File predictions_training/prediction_raw_71.png does not exist\n",
      "File predictions_training/prediction_raw_72.png does not exist\n",
      "File predictions_training/prediction_raw_73.png does not exist\n",
      "File predictions_training/prediction_raw_74.png does not exist\n",
      "File predictions_training/prediction_raw_75.png does not exist\n",
      "File predictions_training/prediction_raw_76.png does not exist\n",
      "File predictions_training/prediction_raw_77.png does not exist\n",
      "File predictions_training/prediction_raw_78.png does not exist\n",
      "File predictions_training/prediction_raw_79.png does not exist\n",
      "File predictions_training/prediction_raw_80.png does not exist\n",
      "File predictions_training/prediction_raw_81.png does not exist\n",
      "File predictions_training/prediction_raw_82.png does not exist\n",
      "File predictions_training/prediction_raw_83.png does not exist\n",
      "File predictions_training/prediction_raw_84.png does not exist\n",
      "File predictions_training/prediction_raw_85.png does not exist\n",
      "File predictions_training/prediction_raw_86.png does not exist\n",
      "File predictions_training/prediction_raw_87.png does not exist\n",
      "File predictions_training/prediction_raw_88.png does not exist\n",
      "File predictions_training/prediction_raw_89.png does not exist\n",
      "File predictions_training/prediction_raw_90.png does not exist\n",
      "File predictions_training/prediction_raw_91.png does not exist\n",
      "File predictions_training/prediction_raw_92.png does not exist\n",
      "File predictions_training/prediction_raw_93.png does not exist\n",
      "File predictions_training/prediction_raw_94.png does not exist\n",
      "File predictions_training/prediction_raw_95.png does not exist\n",
      "File predictions_training/prediction_raw_96.png does not exist\n",
      "File predictions_training/prediction_raw_97.png does not exist\n",
      "File predictions_training/prediction_raw_98.png does not exist\n",
      "File predictions_training/prediction_raw_99.png does not exist\n",
      "File predictions_training/prediction_raw_100.png does not exist\n",
      "2 : extract_labels...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e239159e7539>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#execute phase 1 (train inputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mcnn2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mcnn2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconv_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONV_LAYERS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-15-8400282e7d93>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, phase, conv_layers)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\": extract_labels...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mtrain_labels_filename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'training/groundtruth/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0mtrain_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_labels_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRAINING_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m \u001b[0;31m#iterations count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a2ee8e2e3b3f>\u001b[0m in \u001b[0;36mextract_labels\u001b[0;34m(filename, num_images)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mgt_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADD_INTERCALATED_PATCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Convert to dense 1-hot representation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a2ee8e2e3b3f>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0mgt_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mimg_crop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_imgs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIMG_PATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mADD_INTERCALATED_PATCHES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_images\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgt_patches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Convert to dense 1-hot representation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a2ee8e2e3b3f>\u001b[0m in \u001b[0;36mvalue_to_class\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Assign a label to a patch v\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mvalue_to_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0mforeground_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.25\u001b[0m \u001b[0;31m# percentage of pixels > 1 required to assign a foreground label to a patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## CNN settings:\n",
    "\n",
    "ADD_INTERCALATED_PATCHES = True\n",
    "NEIGHBORHOOD_ANALYSIS = True \n",
    "\n",
    "if NEIGHBORHOOD_ANALYSIS == True:\n",
    "    NEIGHBOR_PIXELS = 5\n",
    "    CONV_FILTER_SIZES = [NEIGHBOR_PIXELS*2+1, 5, 5, 5]\n",
    "else:\n",
    "    CONV_FILTER_SIZES = [5, 5, 5, 5]\n",
    "\n",
    "IMG_PATCH_SIZE = 16\n",
    "CONV_LAYERS=2\n",
    "CONV_FILTER_DEPTHS = [32, 64, 128, 256] #depth of conv_weights[i]\n",
    "POOL_FILTER_STRIDES = [2, 2, 2, 2] #stride for pooling\n",
    "FC1_WEIGHTS_DEPTH = 512 #depth of weights in fully connected 1 (before out)\n",
    "\n",
    "RANDOMIZE_INPUT_PATCHES = True\n",
    "\n",
    "#Learning settings\n",
    "DROPOUT_RATE = 0.5 #amount of nodes we drop during training (0 for 'no dropout')\n",
    "LEARNING_RATE = 0.2\n",
    "DECAY_RATE = 0.95 #decay of step size of gradient descent\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "\n",
    "#execute phase 1 (train inputs)\n",
    "cnn2 = CNN()\n",
    "cnn2.run(phase=2, conv_layers=CONV_LAYERS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mask_to_submission.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "import re\n",
    "\n",
    "foreground_threshold = 0.25 # percentage of pixels > 1 required to assign a foreground label to a patch\n",
    "\n",
    "# assign a label to a patch\n",
    "def patch_to_label(patch):\n",
    "    df = np.mean(patch)\n",
    "    if df > foreground_threshold:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def mask_to_submission_strings(image_filename):\n",
    "    \"\"\"Reads a single image and outputs the strings that should go into the submission file\"\"\"\n",
    "    img_number = int(re.search(r\"\\d+\", image_filename).group(0))\n",
    "    im = mpimg.imread(image_filename)\n",
    "    patch_size = IMG_PATCH_SIZE\n",
    "    for j in range(0, im.shape[1], patch_size):\n",
    "        for i in range(0, im.shape[0], patch_size):\n",
    "            patch = im[i:i + patch_size, j:j + patch_size]\n",
    "            label = patch_to_label(patch)\n",
    "            yield(\"{:03d}_{}_{},{}\".format(img_number, j, i, label))\n",
    "\n",
    "\n",
    "def masks_to_submission(submission_filename, *image_filenames):\n",
    "    \"\"\"Converts images into a submission file\"\"\"\n",
    "    with open(submission_filename, 'w') as f:\n",
    "        f.write('id,prediction\\n')\n",
    "        for fn in image_filenames[0:]:\n",
    "            f.writelines('{}\\n'.format(s) for s in mask_to_submission_strings(fn))\n",
    "\n",
    "\n",
    "submission_filename = 'dummy_submission.csv'\n",
    "image_filenames = []\n",
    "for i in range(1, 51):\n",
    "    image_filename = 'training/groundtruth/satImage_' + '%.3d' % i + '.png'\n",
    "    print image_filename\n",
    "    image_filenames.append(image_filename)\n",
    "    \n",
    "masks_to_submission(submission_filename, *image_filenames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######################## from Log regression code ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
